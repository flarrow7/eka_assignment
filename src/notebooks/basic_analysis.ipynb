{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install fasttext\n",
        "!pip install gensim\n",
        "!pip install autocorrect"
      ],
      "metadata": {
        "id": "kqgQ7_jbRPLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBVCO4OBQ-Yp",
        "outputId": "c13d9725-6d4b-439d-f600-ae6de8597667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx30sNTbQvgn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import re\n",
        "from nltk.corpus import words as nlp_words\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fasttext\n",
        "import gensim\n",
        "import logging\n",
        "import os\n",
        "import nltk.data\n",
        "import string\n",
        "import spacy\n",
        "from autocorrect import Speller\n",
        "import nltk\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmn0EfmQQvgp"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/gdrive/MyDrive/advices_assignment/data/advices_assignment.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "if df.isna().sum()[0]!=0:\n",
        "    print('NULL VALUES PRESENT!!!')\n",
        "df.drop_duplicates(inplace=True, ignore_index=True)\n",
        "df['advice'] = df['advice'].apply(lambda x : literal_eval(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMU9lvNLQvgp"
      },
      "outputs": [],
      "source": [
        "### COMBINING SENTENCES IN ADVICE, LOWERCASING AND DUPLICATE REMOVAL\n",
        "def combine(x):\n",
        "    sent = ''\n",
        "    for i in range(len(x)):\n",
        "        sent += x[i] +', '\n",
        "    return sent[:-2]\n",
        "\n",
        "df['combined'] = df['advice'].apply(lambda x : combine(x))\n",
        "df['count'] = df['combined'].apply(lambda x : len(x.split(' ')))\n",
        "df['count'].quantile(0.95)\n",
        "df['lowercased'] = df['combined'].apply(lambda x : x.lower())\n",
        "df.drop_duplicates(subset=['lowercased'], inplace=True, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLswyUjOQvgp",
        "outputId": "443dfa89-d3dc-43ae-9755-dd8f20be541e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN WORD CORPUS ::  3578\n"
          ]
        }
      ],
      "source": [
        "### GENERATING UNIQUE WORD CORPUS FOR ADVICE DATASET\n",
        "word_list = sum(df['advice'].tolist(), [])\n",
        "corpus = ''\n",
        "for sentence in word_list:\n",
        "    corpus += sentence + ' '\n",
        "\n",
        "corpus = re.sub(r'[^\\w]', ' ', corpus)\n",
        "words = corpus.split(' ')\n",
        "\n",
        "unique_corpus = []\n",
        "for word in words:\n",
        "    word = word.lower()\n",
        "    if word not in unique_corpus:\n",
        "        unique_corpus.append(word)\n",
        "\n",
        "print('LEN WORD CORPUS :: ', len(unique_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI24WzOVQvgq",
        "outputId": "fc2ba63c-342c-46ba-a309-4c55755c4f3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of omitted tokens ::  1659\n"
          ]
        }
      ],
      "source": [
        "### CLEANING THE CORPUS - STEP 1 : GENERATING OMITTION WORD LIST BY CHECKING WITH ENGLISH OCCURRENCES IN NLTK\n",
        "unique_eng_words = list(set(unique_corpus).intersection(set(nlp_words.words())))\n",
        "omitted_words = list(set(unique_corpus) - set(unique_eng_words))\n",
        "\n",
        "print('No of omitted tokens :: ', len(omitted_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2a_4rFwQvgq",
        "outputId": "1f1e246f-1cca-443d-f56f-1bdd28e96f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of omitted tokens ::  200\n"
          ]
        }
      ],
      "source": [
        "### CLEANING THE CORPUS - STEP 2 : REMOVING NON ENGLISH OCCURRENCES\n",
        "\n",
        "corrections = []\n",
        "for w in omitted_words:\n",
        "    w = w.encode('ascii',errors='ignore').decode()\n",
        "    if w!='':\n",
        "        corrections.append(w)\n",
        "\n",
        "omitted_words = list(set(omitted_words) - set(corrections))\n",
        "print('No of omitted tokens :: ', len(omitted_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqmUgy7cQvgr",
        "outputId": "7e5afb0c-318d-4756-f361-25094bb63fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of omitted tokens ::  286\n"
          ]
        }
      ],
      "source": [
        "### CLEANING THE CORPUS - STEP 3 : REMOVING NUMERIC OCCURRENCES\n",
        "\n",
        "omitted_words_ = []\n",
        "for w in corrections:\n",
        "    if w.isdigit():\n",
        "        omitted_words_.append(w)\n",
        "\n",
        "corrections = list(set(corrections) - set(omitted_words_))\n",
        "omitted_words = omitted_words_ + omitted_words\n",
        "print('No of omitted tokens :: ', len(omitted_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo2x8ZMyQvgr",
        "outputId": "283898e7-204d-4bd8-d8c3-7ffa4bfd699f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of omitted tokens ::  404\n"
          ]
        }
      ],
      "source": [
        "### CLEANING THE CORPUS - STEP 5 : REMOVING GARBAGE WORDS WITH REPEATED LETTERS (letter thresh = 2)\n",
        "omitted_words_ = []\n",
        "for w in corrections:\n",
        "    if len(set(w)) <= 2:\n",
        "        omitted_words_.append(w)\n",
        "omitted_words = omitted_words_ + omitted_words\n",
        "corrections = list(set(corrections) - set(omitted_words))\n",
        "print('No of omitted tokens :: ', len(omitted_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I5FR5r9Qvgr"
      },
      "outputs": [],
      "source": [
        "final_unique_corpus = list(set(unique_corpus) - set(omitted_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbZlb5wMQvgr"
      },
      "outputs": [],
      "source": [
        "def remove_garbage(sentence, allowed_word_list):\n",
        "    sentence = sentence.split(' ')\n",
        "    updated_sentence = ''\n",
        "    for word in sentence:\n",
        "        if word in allowed_word_list:\n",
        "            updated_sentence += word + ' '\n",
        "    return updated_sentence[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CcEcxe-Qvgs"
      },
      "outputs": [],
      "source": [
        "df['cleaned_advice'] = df['lowercased'].apply(lambda x : remove_garbage(x, final_unique_corpus))\n",
        "df.drop_duplicates(['cleaned_advice'], inplace=True, ignore_index=True)\n",
        "\n",
        "### dropping one word advices\n",
        "drop_ind = df.query('count==1').index\n",
        "df.drop(index=drop_ind, inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fjaS0uIQvgs"
      },
      "outputs": [],
      "source": [
        "df = df[['advice', 'cleaned_advice', 'count']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_unique_corpus = pd.DataFrame(np.sort(final_unique_corpus), columns=['tokens'])"
      ],
      "metadata": {
        "id": "0lOXudg5TgOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8aIYonmQvgs"
      },
      "outputs": [],
      "source": [
        "final_unique_corpus.to_csv('/content/gdrive/MyDrive/advices_assignment/data/corpus.csv', index=False)\n",
        "df.to_csv('/content/gdrive/MyDrive/advices_assignment/data/cleaned_advices.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ju0eXywyHj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ5ivmY1Qvgs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f1f81b133e116af87e1880ceace82f8225abaafcbf582017243cb7ab9670bf4a"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
